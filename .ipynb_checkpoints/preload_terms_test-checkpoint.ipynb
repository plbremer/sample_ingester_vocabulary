{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b12776f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d70ebd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "class SearchModelCreator:\n",
    "\n",
    "    def __init__(self,panda_containing_training_set_address,output_directory_address,header_subset_definitions_address,ngram_limit_address,extra_terms_address):\n",
    "        self.conglomerate_panda=pd.read_pickle(panda_containing_training_set_address)\n",
    "        self.output_directory_address=output_directory_address\n",
    "        with open(header_subset_definitions_address, 'r') as f:\n",
    "            self.header_definition_json=json.load(f) \n",
    "        with open(ngram_limit_address, 'r') as f:\n",
    "            self.ngram_limit_json=json.load(f) \n",
    "        self.tfidf_matrix_dict=dict()\n",
    "\n",
    "        self.extra_terms_dataframe=pd.read_csv(extra_terms_address,sep='\\t',keep_default_na=False)\n",
    "        self.extra_terms_dataframe['node_id']='extraTerms'\n",
    "        self.extra_terms_dataframe['ontology']='extraTerms'\n",
    "        self.extra_terms_dataframe['use_count']=0 \n",
    "        self.extra_terms_dataframe['vocabulary']=self.extra_terms_dataframe['vocabulary'].str.split(',')\n",
    "        self.extra_terms_dataframe=self.extra_terms_dataframe.explode('vocabulary',ignore_index=True)\n",
    "\n",
    "\n",
    "    def create_tfidf_matrix_per_header_defined(self):\n",
    "        '''\n",
    "        the basic idea is to scroll through all of the headers defined in the header dict\n",
    "        for each header, create the quadruplet (panda, vocab, tfidf, nearestneighbors) using\n",
    "        the subset of possible vocabulary that is specified by subsetting the main conglomerate panda using the\n",
    "        elements in the header dict's lists\n",
    "\n",
    "        if there is no subset defined, then we are starting with a blank vocabulary set. \n",
    "        '''\n",
    "        for i,temp_header in enumerate(self.header_definition_json.keys()): \n",
    "            if i !=3:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            if len(self.header_definition_json[temp_header])==0:\n",
    "                '''\n",
    "                this ended up being a little weirder than i thought it would. there is some duplicated code\n",
    "                '''\n",
    "                temp_conglomerate_panda_subset=self.conglomerate_panda.loc[\n",
    "                    self.conglomerate_panda.node_id.str.contains('thisisanimpossiblestringthatmakesapandawithnorows')\n",
    "                ]\n",
    "\n",
    "                temp_conglomerate_panda_subset.to_pickle(self.output_directory_address+'conglomerate_vocabulary_panda_'+temp_header+'.bin')\n",
    "                temp_model_vocabulary=temp_conglomerate_panda_subset['valid_string'].unique()\n",
    "\n",
    "                temp_model_vocabulary_panda=pd.DataFrame.from_dict(temp_model_vocabulary)\n",
    "                temp_model_vocabulary_panda.to_pickle(self.output_directory_address+'unique_valid_strings_'+temp_header+'.bin')\n",
    "                temp_TfidfVectorizer=TfidfVectorizer(\n",
    "                    analyzer='char',\n",
    "                    ngram_range=self.ngram_limit_json[temp_header]\n",
    "                    #max_df=1,\n",
    "                    #min_df=0.001\n",
    "                )\n",
    "                #no fit transform\n",
    "                with open(self.output_directory_address+'tfidfVectorizer'+'_'+temp_header+'.bin','wb') as fp:\n",
    "                    pickle.dump(temp_TfidfVectorizer,fp)        \n",
    "                continue\n",
    "\n",
    "            #collect all subset_definitions\n",
    "            temp_subset_definitions=self.header_definition_json[temp_header]\n",
    "\n",
    "            temp_panda_subset_list=list()\n",
    "\n",
    "            temp_panda_subset_list.append(\n",
    "                self.add_extra_terms(temp_header) \n",
    "            )\n",
    "\n",
    "\n",
    "            for temp_subset_definition in temp_subset_definitions:\n",
    "                temp_panda_subset_list.append(\n",
    "                    self.conglomerate_panda.loc[\n",
    "                        self.conglomerate_panda.node_id.str.startswith(temp_subset_definition)\n",
    "                    ].copy()\n",
    "                )\n",
    "            \n",
    "            temp_conglomerate_panda_subset=pd.concat(temp_panda_subset_list,axis='index',ignore_index=True)\n",
    "\n",
    "            temp_conglomerate_panda_subset.drop_duplicates(\n",
    "                subset=['valid_string','main_string'],\n",
    "                keep='first',\n",
    "                inplace=True\n",
    "            )\n",
    "\n",
    "            #when the models translates chosen valid strings to nodes, we dont want to ahea access to all of the valid stirngs\n",
    "            #rather just those specified in the ubset. a good example of this is DDT which is a gnee and a pesticide\n",
    "            #so we output this panda\n",
    "            temp_conglomerate_panda_subset.to_pickle(self.output_directory_address+'conglomerate_vocabulary_panda_'+temp_header+'.bin')\n",
    "            \n",
    "            temp_model_vocabulary=temp_conglomerate_panda_subset['valid_string'].unique()\n",
    "\n",
    "            #when the nearest neighbors model gets neighbors, it only knows about indices of points in training set\n",
    "            #we need to map those points to actual strings\n",
    "            # temp_model_vocabulary_dict={\n",
    "            #     'nearest_neighbors_training_index':[i for i in range(len(temp_model_vocabulary))],\n",
    "            #     'valid_strings_unique':temp_model_vocabulary\n",
    "            # }\n",
    "            temp_model_vocabulary_panda=pd.DataFrame.from_dict(temp_model_vocabulary)\n",
    "            temp_model_vocabulary_panda.to_pickle(self.output_directory_address+'unique_valid_strings_'+temp_header+'.bin')\n",
    "\n",
    "            temp_TfidfVectorizer=TfidfVectorizer(\n",
    "                analyzer='char',\n",
    "                ngram_range=self.ngram_limit_json[temp_header],\n",
    "                use_idf=False,\n",
    "                norm=None\n",
    "            )\n",
    "            self.tfidf_matrix_dict[temp_header]=temp_TfidfVectorizer.fit_transform(temp_model_vocabulary)\n",
    "            with open(self.output_directory_address+'tfidfVectorizer'+'_'+temp_header+'.bin','wb') as fp:\n",
    "                pickle.dump(temp_TfidfVectorizer,fp)\n",
    "\n",
    "\n",
    "    def create_NearestNeighbors_model_per_header_defined(self):\n",
    "        for temp_header in self.header_definition_json.keys():\n",
    "        \n",
    "            if len(self.header_definition_json[temp_header])==0:\n",
    "                temp_NN_model=NearestNeighbors(\n",
    "                    n_neighbors=50,\n",
    "                    n_jobs=5,\n",
    "                    metric='cosine'\n",
    "                )          \n",
    "                #no fit, there is no matrix      \n",
    "                with open(self.output_directory_address+'NearestNeighbors'+'_'+temp_header+'.bin','wb') as fp:\n",
    "                    pickle.dump(temp_NN_model,fp)\n",
    "                continue\n",
    "\n",
    "\n",
    "            temp_NN_model=NearestNeighbors(\n",
    "                n_neighbors=50,\n",
    "                n_jobs=5,\n",
    "                metric='cosine'\n",
    "            )\n",
    "            temp_NN_model.fit(self.tfidf_matrix_dict[temp_header])\n",
    "            with open(self.output_directory_address+'NearestNeighbors'+'_'+temp_header+'.bin','wb') as fp:\n",
    "                pickle.dump(temp_NN_model,fp)\n",
    "\n",
    "    def add_extra_terms(self,temp_header):\n",
    "        \n",
    "        to_append=self.extra_terms_dataframe.loc[\n",
    "            ( (self.extra_terms_dataframe['vocabulary']=='all') |\n",
    "             (self.extra_terms_dataframe['vocabulary']==temp_header) ),\n",
    "            ['main_string', 'valid_string', 'node_id', 'ontology', 'use_count']\n",
    "        ].copy()\n",
    "        \n",
    "        return to_append\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f662cf74",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'ngram_range' parameter of TfidfVectorizer must be an instance of 'tuple'. Got [3, 3] instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m my_SearchModelCreator\u001b[38;5;241m=\u001b[39mSearchModelCreator(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/conglomerate_vocabulary_panda/conglomerate_vocabulary_panda.bin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/models/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmy_SearchModelCreator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_tfidf_matrix_per_header_defined\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     my_SearchModelCreator.create_NearestNeighbors_model_per_header_defined()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 113\u001b[0m, in \u001b[0;36mSearchModelCreator.create_tfidf_matrix_per_header_defined\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m temp_model_vocabulary_panda\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_directory_address\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_valid_strings_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mtemp_header\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.bin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    107\u001b[0m temp_TfidfVectorizer\u001b[38;5;241m=\u001b[39mTfidfVectorizer(\n\u001b[1;32m    108\u001b[0m     analyzer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    109\u001b[0m     ngram_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_limit_json[temp_header],\n\u001b[1;32m    110\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    111\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    112\u001b[0m )\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfidf_matrix_dict[temp_header]\u001b[38;5;241m=\u001b[39m\u001b[43mtemp_TfidfVectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_model_vocabulary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_directory_address\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidfVectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mtemp_header\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.bin\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    115\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(temp_TfidfVectorizer,fp)\n",
      "File \u001b[0;32m~/anaconda3/envs/calico_challenge/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2128\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2129\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2130\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2131\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2132\u001b[0m )\n\u001b[0;32m-> 2133\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2136\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/calico_challenge/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1369\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_documents, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterable over raw text documents expected, string object received.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m     )\n\u001b[0;32m-> 1369\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_ngram_range()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_for_unused_params()\n",
      "File \u001b[0;32m~/anaconda3/envs/calico_challenge/lib/python3.8/site-packages/sklearn/base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    593\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 600\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/calico_challenge/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:97\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'ngram_range' parameter of TfidfVectorizer must be an instance of 'tuple'. Got [3, 3] instead."
     ]
    }
   ],
   "source": [
    "    my_SearchModelCreator=SearchModelCreator(\n",
    "        'results/conglomerate_vocabulary_panda/conglomerate_vocabulary_panda.bin',\n",
    "        'results/models/',\n",
    "        'resources/parameter_files/subset_per_heading.json',\n",
    "        'resources/parameter_files/ngram_limits_per_heading.json',\n",
    "        'resources/parameter_files/common_extra_terms.tsv'\n",
    "        \n",
    "    )\n",
    "\n",
    "    my_SearchModelCreator.create_tfidf_matrix_per_header_defined()\n",
    "#     my_SearchModelCreator.create_NearestNeighbors_model_per_header_defined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e42f5633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_string</th>\n",
       "      <th>valid_string</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>node_id</th>\n",
       "      <th>ontology</th>\n",
       "      <th>use_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad libitum</td>\n",
       "      <td>ad libitum</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ad libitum</td>\n",
       "      <td>ad lib</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not available</td>\n",
       "      <td>not collected</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not available</td>\n",
       "      <td>not applicable</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not available</td>\n",
       "      <td>not reported</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not available</td>\n",
       "      <td>not specified</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>not available</td>\n",
       "      <td>null</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>not available</td>\n",
       "      <td>nan</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>not available</td>\n",
       "      <td>n/a</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>not available</td>\n",
       "      <td>not available</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>not available</td>\n",
       "      <td>unknown</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>not available</td>\n",
       "      <td>unidentified</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>not available</td>\n",
       "      <td>none</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>wild-type</td>\n",
       "      <td>wild type</td>\n",
       "      <td>strain</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wild-type</td>\n",
       "      <td>wild type</td>\n",
       "      <td>geneKnockout</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>wild-type</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>strain</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wild-type</td>\n",
       "      <td>wild-type</td>\n",
       "      <td>geneKnockout</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>control</td>\n",
       "      <td>ctrl</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>control</td>\n",
       "      <td>control</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>control</td>\n",
       "      <td>vehicle control</td>\n",
       "      <td>all</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>milligram per kilogram</td>\n",
       "      <td>milligram per kilogram</td>\n",
       "      <td>drugDoseUnit</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>milligram per kilogram</td>\n",
       "      <td>mg/kg</td>\n",
       "      <td>drugDoseUnit</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>extraTerms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               main_string            valid_string    vocabulary     node_id  \\\n",
       "0               ad libitum              ad libitum           all  extraTerms   \n",
       "1               ad libitum                  ad lib           all  extraTerms   \n",
       "2            not available           not collected           all  extraTerms   \n",
       "3            not available          not applicable           all  extraTerms   \n",
       "4            not available            not reported           all  extraTerms   \n",
       "5            not available           not specified           all  extraTerms   \n",
       "6            not available                    null           all  extraTerms   \n",
       "7            not available                     nan           all  extraTerms   \n",
       "8            not available                     n/a           all  extraTerms   \n",
       "9            not available           not available           all  extraTerms   \n",
       "10           not available                 unknown           all  extraTerms   \n",
       "11           not available            unidentified           all  extraTerms   \n",
       "12           not available                    none           all  extraTerms   \n",
       "13               wild-type               wild type        strain  extraTerms   \n",
       "14               wild-type               wild type  geneKnockout  extraTerms   \n",
       "15               wild-type               wild-type        strain  extraTerms   \n",
       "16               wild-type               wild-type  geneKnockout  extraTerms   \n",
       "17                 control                    ctrl           all  extraTerms   \n",
       "18                 control                 control           all  extraTerms   \n",
       "19                 control         vehicle control           all  extraTerms   \n",
       "20  milligram per kilogram  milligram per kilogram  drugDoseUnit  extraTerms   \n",
       "21  milligram per kilogram                   mg/kg  drugDoseUnit  extraTerms   \n",
       "\n",
       "      ontology  use_count  \n",
       "0   extraTerms          0  \n",
       "1   extraTerms          0  \n",
       "2   extraTerms          0  \n",
       "3   extraTerms          0  \n",
       "4   extraTerms          0  \n",
       "5   extraTerms          0  \n",
       "6   extraTerms          0  \n",
       "7   extraTerms          0  \n",
       "8   extraTerms          0  \n",
       "9   extraTerms          0  \n",
       "10  extraTerms          0  \n",
       "11  extraTerms          0  \n",
       "12  extraTerms          0  \n",
       "13  extraTerms          0  \n",
       "14  extraTerms          0  \n",
       "15  extraTerms          0  \n",
       "16  extraTerms          0  \n",
       "17  extraTerms          0  \n",
       "18  extraTerms          0  \n",
       "19  extraTerms          0  \n",
       "20  extraTerms          0  \n",
       "21  extraTerms          0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_SearchModelCreator.extra_terms_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804cb431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00662a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa1774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dcd4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
